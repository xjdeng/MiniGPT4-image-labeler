{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVpDEBtwAUl7CFA8XqYltk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdeng/MiniGPT4-image-labeler/blob/main/server_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Base Requirements"
      ],
      "metadata": {
        "id": "qOvTZW63TT7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aJdvu1y4P_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "%cd text-generation-webui\n",
        "!git log -n 1 --format=%H\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/Wojtab/minigpt-4-pipeline extensions/multimodal/pipelines/minigpt-4-pipeline\n",
        "!pip install -r extensions/multimodal/pipelines/minigpt-4-pipeline/requirements.txt\n",
        "!pip install -U gradio-client flask-cloudflared"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install GPTQ Requirements"
      ],
      "metadata": {
        "id": "e-m7fBXFTcm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "%cd GPTQ-for-LLaMa\n",
        "!git log -n 1 --format=%H\n",
        "!python setup_cuda.py install\n",
        "%cd ..\n",
        "!cp -r GPTQ-for-LLaMa/* ."
      ],
      "metadata": {
        "id": "OOIcOfv_y_CK",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Model\n",
        "You can go to https://huggingface.co and pick another Vicuna-based model if you wish and plug the path (without the https://huggingface.co part) below. Note that you may need an A100 Premium Colab GPU for models larger than 13B."
      ],
      "metadata": {
        "id": "vDV-95lyTj7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelstr = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\" #@param {type:\"string\"}\n",
        "\n",
        "!python download-model.py $modelstr\n",
        "modelfolder = modelstr.replace(\"/\",\"_\")"
      ],
      "metadata": {
        "id": "XHzhN92nzlZ9",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the server\n",
        "Take note of the URL after \"**Starting non-streaming server at public url**\".\n",
        "You may see some error messages but if you don't see a URL after that text above, then please stop and restart the cell below. You'll need to copy the URL of the non-streaming server and paste it into the appropriate fields in the Client notebook. You may need to go to View -> Show/hide output in the next cell.\n",
        "\n",
        "After you're done running the server, please go up to **Runtime -> Disconnect and delete runtime**.\n",
        "\n",
        "Otherwise, the server will keep running idle until Google shuts it down and Google may restrict your Colab access in the future if it happens frequently."
      ],
      "metadata": {
        "id": "_GNSeuEYT5x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!python server.py --extensions multimodal --multimodal-pipeline minigpt4-13b --share --chat --loader gptq-for-llama --model_type llama --model $modelfolder --wbits 4 --groupsize 128 --auto-devices --public --public-api --sdp-attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj4mJGMvzqCy",
        "outputId": "3e6cd3e4-a70e-4f72-853b-aa1f411224f8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-15 20:58:56 WARNING:\u001b[33mThe gradio \"share link\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\u001b[0m\n",
            "2023-07-15 20:58:58.451085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "2023-07-15 20:59:02 INFO:\u001b[32mLoading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\u001b[0m\n",
            "2023-07-15 20:59:02 INFO:\u001b[32mFound the following quantized model: models/TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ/Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\u001b[0m\n",
            "2023-07-15 20:59:52 WARNING:\u001b[33mmodels/TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ/tokenizer_config.json is different from the original LlamaTokenizer file. It is either customized or outdated.\u001b[0m\n",
            "2023-07-15 20:59:52 WARNING:\u001b[33mmodels/TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ/special_tokens_map.json is different from the original LlamaTokenizer file. It is either customized or outdated.\u001b[0m\n",
            "2023-07-15 20:59:52 INFO:\u001b[32mReplaced attention with sdp_attention\u001b[0m\n",
            "2023-07-15 20:59:52 INFO:\u001b[32mLoaded the model in 49.86 seconds.\n",
            "\u001b[0m\n",
            "2023-07-15 20:59:52 INFO:\u001b[32mLoading the extension \"multimodal\"...\u001b[0m\n",
            "2023-07-15 20:59:52 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
            " * Downloading cloudflared for Linux x86_64...\n",
            " * Downloading cloudflared for Linux x86_64...\n",
            "Downloading (…)trained_minigpt4.pth: 100% 47.4M/47.4M [00:00<00:00, 234MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 547kB/s]\n",
            "Starting non-streaming server at public url https://speaks-valuation-reliability-petroleum.trycloudflare.com/api\n",
            "Starting streaming server at public url wss://formed-hugh-gauge-rouge.trycloudflare.com/api/v1/stream\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 129kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.49MB/s]\n",
            "Loading VIT\n",
            "100% 1.89G/1.89G [01:18<00:00, 25.6MB/s]\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n",
            "100% 413M/413M [00:19<00:00, 21.9MB/s]\n",
            "Loading Q-Former Done\n",
            "2023-07-15 21:02:01 INFO:\u001b[32mMultimodal: loaded pipeline minigpt4-13b from pipelines/minigpt-4-pipeline (MiniGPT4_13b_Pipeline)\u001b[0m\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://4bf5cf9498974102fa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ]
    }
  ]
}