{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN95e2MBY4I3u6orvfwtlq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdeng/MiniGPT4-image-labeler/blob/main/server_failsafe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Base Requirements"
      ],
      "metadata": {
        "id": "qOvTZW63TT7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aJdvu1y4P_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "%cd text-generation-webui\n",
        "!git checkout 0e6295886d2d0e9309c8223201e8a4bd55e4735c\n",
        "!git log -n 1 --format=%H\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/Wojtab/minigpt-4-pipeline extensions/multimodal/pipelines/minigpt-4-pipeline\n",
        "!pip install -r extensions/multimodal/pipelines/minigpt-4-pipeline/requirements.txt\n",
        "!pip install -U gradio-client flask-cloudflared"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install GPTQ Requirements"
      ],
      "metadata": {
        "id": "e-m7fBXFTcm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "%cd GPTQ-for-LLaMa\n",
        "!git checkout 53a2aeac935b7eef0e1451eb7b465fea4f318e7a\n",
        "!git log -n 1 --format=%H\n",
        "!python setup_cuda.py install\n",
        "%cd ..\n",
        "!cp -r GPTQ-for-LLaMa/* ."
      ],
      "metadata": {
        "id": "OOIcOfv_y_CK",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Model\n",
        "You can go to https://huggingface.co and pick another Vicuna-based model if you wish and plug the path (without the https://huggingface.co part) below. Note that you may need an A100 Premium Colab GPU for models larger than 13B."
      ],
      "metadata": {
        "id": "vDV-95lyTj7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelstr = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\" #@param {type:\"string\"}\n",
        "\n",
        "!python download-model.py $modelstr\n",
        "modelfolder = modelstr.replace(\"/\",\"_\")"
      ],
      "metadata": {
        "id": "XHzhN92nzlZ9",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the server\n",
        "Take note of the URL after \"**Starting non-streaming server at public url**\".\n",
        "You may see some error messages but if you don't see a URL after that text above, then please stop and restart the cell below. You'll need to copy the URL of the non-streaming server and paste it into the appropriate fields in the Client notebook. You may need to go to View -> Show/hide output in the next cell.\n",
        "\n",
        "After you're done running the server, please go up to **Runtime -> Disconnect and delete runtime**.\n",
        "\n",
        "Otherwise, the server will keep running idle until Google shuts it down and Google may restrict your Colab access in the future if it happens frequently."
      ],
      "metadata": {
        "id": "_GNSeuEYT5x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!python server.py --extensions multimodal --multimodal-pipeline minigpt4-13b --share --chat --loader gptq-for-llama --model_type llama --model $modelfolder --wbits 4 --groupsize 128 --auto-devices --public --public-api --sdp-attention"
      ],
      "metadata": {
        "id": "hj4mJGMvzqCy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}